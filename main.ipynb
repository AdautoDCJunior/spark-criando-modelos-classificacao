{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark: SparkSession = SparkSession \\\n",
    "  .builder \\\n",
    "  .master('local[*]') \\\n",
    "  .appName('classification with spark') \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/dados_clientes.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('Churn').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = [\n",
    "  'Churn',\n",
    "  'Conjuge',\n",
    "  'Dependentes',\n",
    "  'TelefoneFixo',\n",
    "  'MaisDeUmaLinhaTelefonica',\n",
    "  'SegurancaOnline',\n",
    "  'BackupOnline',\n",
    "  'SeguroDispositivo',\n",
    "  'SuporteTecnico',\n",
    "  'TVaCabo',\n",
    "  'StreamingFilmes',\n",
    "  'ContaCorreio'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = [f.when(f.col(c) == 'Sim', 1).otherwise(0).alias(c) for c in binary_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[all_columns.insert(0, c) if c not in binary_columns else None for c in reversed(df.columns)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.select(all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.select('Internet', 'TipoContrato', 'MetodoPagamento').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internet = dataset.groupBy('id').pivot('Internet').agg(f.lit(1)).fillna(0)\n",
    "internet = internet \\\n",
    "  .select([f.col(c).alias(f'Internet_{c}') if c != 'id' else f.col(c) for c in internet.columns])\n",
    "\n",
    "\n",
    "contract_type = dataset.groupBy('id').pivot('TipoContrato').agg(f.lit(1)).fillna(0)\n",
    "contract_type = contract_type \\\n",
    "  .select([f.col(c).alias(f'TipoContrato_{c}') if c != 'id' else f.col(c) for c in contract_type.columns])\n",
    "\n",
    "\n",
    "payment_method = dataset.groupBy('id').pivot('MetodoPagamento').agg(f.lit(1)).fillna(0)\n",
    "payment_method = payment_method \\\n",
    "  .select([f.col(c).alias(f'MetodoPagamento_{c}') if c != 'id' else f.col(c) for c in payment_method.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['Internet', 'TipoContrato', 'MetodoPagamento']\n",
    "\n",
    "dataset = dataset \\\n",
    "  .join(internet, 'id', how='inner') \\\n",
    "  .join(contract_type, 'id', how='inner') \\\n",
    "  .join(payment_method, 'id', how='inner') \\\n",
    "  .drop(*drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.withColumnRenamed('Churn', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['label', 'id']\n",
    "x = []\n",
    "\n",
    "[x.append(c) if c not in drop_columns else None for c in dataset.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=x, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prep = assembler.transform(dataset).select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prep.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = dataset_prep.randomSplit([0.7, 0.3], seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr_test = model_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_lr_summary = model_lr.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_lr_summary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_dtc = dtc.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dtc_training = modelo_dtc.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dtc_test = modelo_dtc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_test_tp = predictions_lr_test.select('label', 'prediction').where((f.col('label') == 1) & (f.col('prediction') == 1)).count()\n",
    "lr_test_tn = predictions_lr_test.select('label', 'prediction').where((f.col('label') == 0) & (f.col('prediction') == 0)).count()\n",
    "lr_test_fp = predictions_lr_test.select('label', 'prediction').where((f.col('label') == 0) & (f.col('prediction') == 1)).count()\n",
    "lr_test_fn = predictions_lr_test.select('label', 'prediction').where((f.col('label') == 1) & (f.col('prediction') == 0)).count()\n",
    "\n",
    "dtc_training_tp = predictions_dtc_training.select('label', 'prediction').where((f.col('label') == 1) & (f.col('prediction') == 1)).count()\n",
    "dtc_training_tn = predictions_dtc_training.select('label', 'prediction').where((f.col('label') == 0) & (f.col('prediction') == 0)).count()\n",
    "dtc_training_fp = predictions_dtc_training.select('label', 'prediction').where((f.col('label') == 0) & (f.col('prediction') == 1)).count()\n",
    "dtc_training_fn = predictions_dtc_training.select('label', 'prediction').where((f.col('label') == 1) & (f.col('prediction') == 0)).count()\n",
    "\n",
    "dtc_test_tp = predictions_dtc_test.select('label', 'prediction').where((f.col('label') == 1) & (f.col('prediction') == 1)).count()\n",
    "dtc_test_tn = predictions_dtc_test.select('label', 'prediction').where((f.col('label') == 0) & (f.col('prediction') == 0)).count()\n",
    "dtc_test_fp = predictions_dtc_test.select('label', 'prediction').where((f.col('label') == 0) & (f.col('prediction') == 1)).count()\n",
    "dtc_test_fn = predictions_dtc_test.select('label', 'prediction').where((f.col('label') == 1) & (f.col('prediction') == 0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('- Relatorio dos desempenhos dos modelos')\n",
    "print('='*80)\n",
    "print('- Modelo de regressão lógistica')\n",
    "print('-'*80)\n",
    "print(\"- Acurácia: %f\" % training_lr_summary.accuracy)\n",
    "print(\"- Precisão: %f\" % training_lr_summary.precisionByLabel[1])\n",
    "print(\"- Recall: %f\" % training_lr_summary.recallByLabel[1])\n",
    "print(\"- F1: %f\" % training_lr_summary.fMeasureByLabel()[1])\n",
    "print('-'*80)\n",
    "print('- Resultado com os dados de teste')\n",
    "print('-'*80)\n",
    "print(f'- Churn   : {lr_test_tp} acertos | {lr_test_fp} erros')\n",
    "print(f'- No-Churn: {lr_test_tn} acertos | {lr_test_fn} erros')\n",
    "print('='*80)\n",
    "print('- Modelo de Árvore de decisão')\n",
    "print('-'*80)\n",
    "print('- Treino')\n",
    "print('-'*80)\n",
    "print(\"- Acurácia: %f\" % evaluator.evaluate(predictions_dtc_training, {evaluator.metricName: \"accuracy\"}))\n",
    "print(\"- Precisão: %f\" % evaluator.evaluate(predictions_dtc_training, {evaluator.metricName: \"precisionByLabel\", evaluator.metricLabel: 1}))\n",
    "print(\"- Recall: %f\" % evaluator.evaluate(predictions_dtc_training, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 1}))\n",
    "print(\"- F1: %f\" % evaluator.evaluate(predictions_dtc_training, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 1}))\n",
    "print('-'*80)\n",
    "print('- Resultado com os dados de treino')\n",
    "print('-'*80)\n",
    "print(f'- Churn   : {dtc_training_tp} acertos | {dtc_training_fp} erros')\n",
    "print(f'- No-Churn: {dtc_training_tn} acertos | {dtc_training_fn} erros')\n",
    "print('-'*80)\n",
    "print('- Teste')\n",
    "print('-'*80)\n",
    "print(\"- Acurácia: %f\" % evaluator.evaluate(predictions_dtc_test, {evaluator.metricName: \"accuracy\"}))\n",
    "print(\"- Precisão: %f\" % evaluator.evaluate(predictions_dtc_test, {evaluator.metricName: \"precisionByLabel\", evaluator.metricLabel: 1}))\n",
    "print(\"- Recall: %f\" % evaluator.evaluate(predictions_dtc_test, {evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 1}))\n",
    "print(\"- F1: %f\" % evaluator.evaluate(predictions_dtc_test, {evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 1}))\n",
    "print('-'*80)\n",
    "print('- Resultado com os dados de teste')\n",
    "print('-'*80)\n",
    "print(f'- Churn   : {dtc_test_tp} acertos | {dtc_test_fp} erros')\n",
    "print(f'- No-Churn: {dtc_test_tn} acertos | {dtc_test_fn} erros')\n",
    "print('='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
